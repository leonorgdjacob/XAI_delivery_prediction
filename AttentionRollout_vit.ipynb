{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Grad-CAM applied to MedViT model for binary classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQy2-7acwuQb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "import torchvision.utils as vutils\n",
        "import torchvision.transforms.functional as TF\n",
        "from MedViT import MedViT_base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MembvQS3w3Bv"
      },
      "outputs": [],
      "source": [
        "# Function to create and load trained MedViT model\n",
        "def load_medvit_model(weight_path, num_classes=2, device=torch.device(\"cpu\")):\n",
        "    # Cria modelo com a mesma estrutura usada no treino\n",
        "    model = MedViT_base(pretrained=False, num_classes=1000)\n",
        "\n",
        "    # Carrega pesos do modelo treinado\n",
        "    state_dict = torch.load(weight_path, map_location=device)\n",
        "\n",
        "    # Atualiza manualmente a última camada\n",
        "    in_features = model.proj_head[0].in_features\n",
        "    model.proj_head = torch.nn.Sequential(\n",
        "        torch.nn.Linear(in_features, num_classes)\n",
        "    )\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlHbwPlCxE6C"
      },
      "outputs": [],
      "source": [
        "class VITAttentionGradRolloutWithClassAttribution:\n",
        "    def __init__(self, model, attention_module_path=\"features.19\", discard_ratio=0.9):\n",
        "        self.model = model\n",
        "        self.discard_ratio = discard_ratio\n",
        "        self.attentions = []\n",
        "        self.attention_gradients = []\n",
        "        self.handles = []\n",
        "\n",
        "        for name, module in self.model.named_modules():\n",
        "            if name.startswith(\"features.\"):\n",
        "                print(f\"{name:<20} {type(module)}\")\n",
        "\n",
        "\n",
        "        # Localizar e registar hooks no módulo correto\n",
        "        for name, module in self.model.named_modules():\n",
        "            if name == attention_module_path:\n",
        "                print(f\">> HOOKING: {name} → {type(module)}\")\n",
        "                print(f\">> Has 'last_attn'? {'last_attn' in dir(module)}\")\n",
        "\n",
        "                self._register_custom_hook(module)\n",
        "\n",
        "    def _register_custom_hook(self, module):\n",
        "        def forward_hook(mod, input, output):\n",
        "            attn = mod.last_attn.detach()\n",
        "            self.attentions.append(attn)\n",
        "            attn.requires_grad_(True)\n",
        "            attn.retain_grad()\n",
        "            attn.register_hook(lambda grad: self.attention_gradients.append(grad.detach()))\n",
        "\n",
        "        handle = module.register_forward_hook(forward_hook)\n",
        "        self.handles.append(handle)\n",
        "\n",
        "\n",
        "    def rollout(self, input_tensor, target_index=None):\n",
        "        self.attentions = []\n",
        "        self.attention_gradients = []\n",
        "\n",
        "        output = self.model(input_tensor)\n",
        "        if isinstance(output, tuple):\n",
        "            output = output[0]\n",
        "        pred_class = output.argmax(dim=1).item() if target_index is None else target_index\n",
        "\n",
        "        self.model.zero_grad()\n",
        "        one_hot = torch.zeros_like(output)\n",
        "        one_hot[0, pred_class] = 1\n",
        "        output.backward(gradient=one_hot)\n",
        "\n",
        "        result = None\n",
        "\n",
        "        with torch.no_grad():\n",
        "            print(\">> ATTENTIONS CAPTURED:\", len(self.attentions))\n",
        "\n",
        "            for attention, grad in zip(self.attentions, self.attention_gradients):\n",
        "                if attention.shape != grad.shape:\n",
        "                    print(f\"Skipping incompatible pair: attention {attention.shape}, grad {grad.shape}\")\n",
        "                    continue\n",
        "\n",
        "                weights = grad\n",
        "                attn_heads_fused = (attention * weights).mean(dim=1)\n",
        "                attn_heads_fused = torch.clamp(attn_heads_fused, min=0)\n",
        "\n",
        "                flat = attn_heads_fused.view(attn_heads_fused.size(0), -1)\n",
        "                _, indices = flat.topk(int(flat.size(-1) * self.discard_ratio), dim=-1, largest=False)\n",
        "                flat.scatter_(1, indices, 0)\n",
        "                attn_heads_fused = flat.view_as(attn_heads_fused)\n",
        "\n",
        "                token_dim = attn_heads_fused.size(-1)\n",
        "                I = torch.eye(token_dim, device=attn_heads_fused.device).unsqueeze(0)\n",
        "                a = (attn_heads_fused + I) / 2\n",
        "                a = a / a.sum(dim=-1, keepdim=True)\n",
        "\n",
        "                if result is None:\n",
        "                    result = a\n",
        "                else:\n",
        "                    result = torch.matmul(a, result)\n",
        "\n",
        "        # Remove class token\n",
        "        mask = result[0, 0]\n",
        "        if mask.shape[0] in [65, 197, 577]:  # caso típico ViT\n",
        "            mask = mask[1:]\n",
        "\n",
        "        num_tokens = mask.shape[0]\n",
        "        side = int(num_tokens ** 0.5)\n",
        "        mask = mask[:side * side].reshape(1, 1, side, side)\n",
        "        mask = torch.nn.functional.interpolate(mask, size=(224, 224), mode=\"bilinear\", align_corners=False)\n",
        "        mask = mask.squeeze().cpu().numpy()\n",
        "        mask = (mask - mask.min()) / (mask.max() + 1e-8)\n",
        "        return mask\n",
        "\n",
        "    def clear_hooks(self):\n",
        "        for handle in self.handles:\n",
        "            handle.remove()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvYDMrgtDGDo"
      },
      "outputs": [],
      "source": [
        "# Utility to overlay heatmap\n",
        "def overlay_heatmap(gray_img, heatmap):\n",
        "    # Redimensionar heatmap para o tamanho da imagem original\n",
        "    heatmap_resized = cv2.resize(heatmap, (gray_img.shape[1], gray_img.shape[0]), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "    heatmap_uint8 = np.uint8(255 * heatmap_resized)\n",
        "    heatmap_color = cv2.applyColorMap(heatmap_uint8, cv2.COLORMAP_JET)\n",
        "\n",
        "    gray_bgr = cv2.cvtColor(gray_img, cv2.COLOR_GRAY2BGR)\n",
        "    overlay = cv2.addWeighted(heatmap_color, 0.5, gray_bgr, 0.5, 0)\n",
        "    return overlay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ceRFezoSDN8p"
      },
      "outputs": [],
      "source": [
        "# Utility to add colored frame\n",
        "def add_colored_frame(image, color, thickness=20):\n",
        "    return cv2.copyMakeBorder(image, thickness, thickness, thickness, thickness, cv2.BORDER_CONSTANT, value=color)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwB2aywJDQog"
      },
      "outputs": [],
      "source": [
        "# Apply Grad-Rollout on folder of images\n",
        "def apply_gradrollout_and_save(model, rollout, input_folder, output_folder, class_names, grid_image_size=(80, 80)):\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    transform = T.Compose([\n",
        "        T.Resize((224, 224)),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean=[.5], std=[.5])\n",
        "    ])\n",
        "\n",
        "    all_heatmaps = []\n",
        "    correct_heatmaps = []\n",
        "    incorrect_heatmaps = []\n",
        "    correct_tensors = []\n",
        "    incorrect_tensors = []\n",
        "\n",
        "    for img_name in os.listdir(input_folder):\n",
        "        if not img_name.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
        "            continue\n",
        "\n",
        "        img_path = os.path.join(input_folder, img_name)\n",
        "\n",
        "        #Load image\n",
        "        image = Image.open(img_path).convert(\"L\")\n",
        "\n",
        "        #Keep a copy of original grayscale image\n",
        "        original = np.array(image)\n",
        "        #original_images.append(original_np)\n",
        "\n",
        "        input_tensor = transform(image.convert(\"RGB\")).unsqueeze(0)\n",
        "\n",
        "        #vit-explain\n",
        "        with torch.no_grad():\n",
        "            output = model(input_tensor)\n",
        "            if isinstance(output, tuple):\n",
        "                output = output[0]\n",
        "            pred_class = output.argmax(dim=1).item()\n",
        "\n",
        "        mask = rollout.rollout(input_tensor, target_index=pred_class)\n",
        "        all_heatmaps.append(mask)\n",
        "\n",
        "        gt_label = 0 if \"Cesarean\" in img_name else 1  # example: ground truth from filename\n",
        "        frame_color = (0, 255, 0) if pred_class == gt_label else (0, 0, 255)\n",
        "        if pred_class == gt_label:\n",
        "            correct_heatmaps.append(mask)\n",
        "        else:\n",
        "            incorrect_heatmaps.append(mask)\n",
        "\n",
        "        overlay = overlay_heatmap(original, mask)\n",
        "        framed = add_colored_frame(overlay, frame_color)\n",
        "        cv2.imwrite(os.path.join(output_folder, f\"heatmap_{img_name}\"), framed)\n",
        "\n",
        "        pil_resized = Image.fromarray(cv2.cvtColor(framed, cv2.COLOR_BGR2RGB)).resize(grid_image_size)\n",
        "        tensor_img = TF.to_tensor(pil_resized)\n",
        "        if pred_class == gt_label:\n",
        "            correct_tensors.append(tensor_img)\n",
        "        else:\n",
        "            incorrect_tensors.append(tensor_img)\n",
        "\n",
        "    final_grid = incorrect_tensors + correct_tensors\n",
        "    if final_grid:\n",
        "        grid = vutils.make_grid(final_grid, nrow=20, padding=5, normalize=True)\n",
        "        vutils.save_image(grid, os.path.join(output_folder, \"grid_overlay_ordered.jpg\"))\n",
        "\n",
        "    for tag, maps in zip([\"correct\", \"incorrect\"], [correct_heatmaps, incorrect_heatmaps]):\n",
        "        if maps:\n",
        "            mean_map = np.mean(np.stack(maps), axis=0)\n",
        "            mean_map -= mean_map.min()\n",
        "            mean_map /= (mean_map.max() + 1e-8)\n",
        "            mean_uint8 = np.uint8(255 * mean_map)\n",
        "            mean_colored = cv2.applyColorMap(mean_uint8, cv2.COLORMAP_JET)\n",
        "            h, w = original.shape[:2]\n",
        "            cv2.imwrite(os.path.join(output_folder, f\"mean_heatmap_{tag}_blocky.jpg\"), cv2.resize(mean_colored, (w, h), interpolation=cv2.INTER_NEAREST))\n",
        "            cv2.imwrite(os.path.join(output_folder, f\"mean_heatmap_{tag}_smooth.jpg\"), cv2.resize(mean_colored, (w, h), interpolation=cv2.INTER_CUBIC))\n",
        "\n",
        "    rollout.clear_hooks()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    input_folder = \"C:/Users/anale/OneDrive/Documentos/Universidade/TESE/image-dataset/dataset_images_cv_1/Abdomen_/test/Cesarean Birth\"  # Folder with images to explain\n",
        "    output_folder = \"C:/Users/anale/OneDrive/Documentos/Universidade/TESE/RESULTS/X_ABDOMEN_cesarean_test_cv1_medvit_pretrained\"   # Where to save heatmaps\n",
        "    weight_path = \"C:/Users/anale/OneDrive/Documentos/Universidade/TESE/MSc-Thesis/model_paths/abdomen_cv1_medvit_pretrained_best-model.pth\"  \n",
        "    #abdomen_cv1_medvit_pretrained_best-model\n",
        "    class_names = [\"Cesarean Birth\", \"Vaginal Birth\"]\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Load trained MedViT model\n",
        "    model = load_medvit_model(weight_path, num_classes=len(class_names), device=device)\n",
        "\n",
        "    # Create Grad-Rollout explainer\n",
        "    rollout = VITAttentionGradRolloutWithClassAttribution(model)\n",
        "\n",
        "    # Apply and save explanations\n",
        "    apply_gradrollout_and_save(model, rollout, input_folder, output_folder, class_names)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "leonorjacob-tese",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
